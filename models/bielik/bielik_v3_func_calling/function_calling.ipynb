{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2566c6c8",
   "metadata": {},
   "source": [
    "# Bielik v3 4.5B - Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "336198fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kacper/Documents/Learn/ai_playground/models/bielik/bielik_v3_func_calling/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98baf1d",
   "metadata": {},
   "source": [
    "Sprawdzenie dostępności GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24700463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "if device == \"cpu\":\n",
    "    print(\"You dont't have access to gpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3e543c",
   "metadata": {},
   "source": [
    "## Prepare Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dd5a2f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Hugging Face enabled? True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "hf_token = getpass(\"Enter your Hugging Face token.\")\n",
    "enable_hf = bool(hf_token)\n",
    "print(f\"Is Hugging Face enabled? {enable_hf}\")\n",
    "\n",
    "if enable_hf:\n",
    "    os.environ[\"HF_TOKEN\"] = hf_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6e9e42",
   "metadata": {},
   "source": [
    "## Download model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6b8c2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.31s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"speakleash/Bielik-4.5B-v3.0-Instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    low_cpu_mem_usage=True,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef0101d",
   "metadata": {},
   "source": [
    "### First try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f726effb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s><|im_start|> user\n",
      "Przepis na jajecznicę w dwóch zdaniach<|im_end|> \n",
      "\n",
      "1. Na patelni rozgrzej masło, dodaj posiekaną cebulę i smaż, aż zmięknie.\n",
      "2. Wbij jajka, dopraw solą i pieprzem, mieszaj, aż się zetną.<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Przepis na jajecznicę w dwóch zdaniach\"\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "input_ids = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(input_ids, max_new_tokens=1000)\n",
    "response = tokenizer.batch_decode(outputs)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f5b3e6",
   "metadata": {},
   "source": [
    "## Define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59bae4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_weather(city):\n",
    "    \"\"\"Sprawdza pogodę w podanym mieście.\"\"\"\n",
    "    weather = {\n",
    "        \"warszawa\": {\"temperatura\": \"12°C\", \"opis\": \"słonecznie\"},\n",
    "        \"kraków\": {\"temperatura\": \"10°C\", \"opis\": \"zachmurzenie\"},\n",
    "        \"gdańsk\": {\"temperatura\": \"18°C\", \"opis\": \"deszczowo\"},\n",
    "        \"bydgoszcz\": {\"temperatura\": \"23°C\", \"opis\": \"słonecznie\"}\n",
    "    }\n",
    "\n",
    "    city = city.lower()\n",
    "    if city in weather:\n",
    "        return weather[city]\n",
    "    else:\n",
    "        return {\"błąd\": f\"Nie znaleziono pogody dla miasta {city}\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44281995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'temperatura': '12°C', 'opis': 'słonecznie'}\n"
     ]
    }
   ],
   "source": [
    "print(get_weather(\"Warszawa\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b0e763",
   "metadata": {},
   "source": [
    "## Funtion format for LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "47a63f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function definition for model:\n",
      "[{\"name\": \"get_weather\", \"description\": \"Sprawdza aktualną pogodę w podanym polskim mieście\", \"parameters\": {\"type\": \"object\", \"properties\": {\"city\": \"string\", \"description\": \"Nazwa miasta w Polsce, np. Warszawa\"}}, \"required\": [\"city\"]}]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Sprawdza aktualną pogodę w podanym polskim mieście\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"city\": \"string\",\n",
    "                \"description\": \"Nazwa miasta w Polsce, np. Warszawa\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"city\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "tools_json = json.dumps(tools, ensure_ascii=False)\n",
    "print(\"Function definition for model:\")\n",
    "print(tools_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93aa58b",
   "metadata": {},
   "source": [
    "## Query to model with function definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15871ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Surowa odpowiedź modelu:\n",
      "<s><|im_start|>  system\n",
      "Jesteś pomocnym asystentem AI. Gdy potrzebujesz dodatkowych informacji, możesz używać dostępnych funkcji.<|im_end|> \n",
      "<|im_start|>  user\n",
      "Jaka jest pogoda w Bydgoszczy?\n",
      "\n",
      "Dostępne funkcje: [{\"name\": \"get_weather\", \"description\": \"Sprawdza aktualną pogodę w podanym polskim mieście\", \"parameters\": {\"type\": \"object\", \"properties\": {\"city\": \"string\", \"description\": \"Nazwa miasta w Polsce, np. Warszawa\"}}, \"required\": [\"city\"]}]<|im_end|> \n",
      "<|im_start|>  assistant\n",
      "Aby uzyskać informacje o pogodzie w Bydgoszczy, użyję funkcji `get_weather` z miastem \"Bydgoszcz\".\n",
      "\n",
      "<tool_call> {\"name\": \"get_weather\", \"arguments\": {\"city\": \"Bydgoszcz\"}}</tool_call><|im_end|>\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Jaka jest pogoda w Bydgoszczy?\"\n",
    "\n",
    "system = \"Jesteś pomocnym asystentem AI. Gdy potrzebujesz dodatkowych informacji, możesz używać dostępnych funkcji.\"\n",
    "\n",
    "main_prompt = f\"{prompt}\\n\\nDostępne funkcje: {tools_json}\"\n",
    " \n",
    "formatted_prompt = f\"<|im_start|> system\\n{system}<|im_end|>\\n<|im_start|> user\\n{main_prompt}<|im_end|>\\n<|im_start|> assistant\\n\"\n",
    "\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_new_tokens=500)\n",
    "\n",
    "raw_response = tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "print(\"Surowa odpowiedź modelu:\")\n",
    "print(raw_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eabc56bf",
   "metadata": {},
   "source": [
    "## Extract function call from response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1022fb98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wyodrębnione wywołanie funkcji:\n",
      "{\n",
      "  \"name\": \"get_weather\",\n",
      "  \"arguments\": {\n",
      "    \"city\": \"Bydgoszcz\"\n",
      "  },\n",
      "  \"function\": \"get_weather\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def get_function_calling(text):\n",
    "    pattern = r\"<tool_call>\\s*(.*?)\\s*</tool_call>\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    \n",
    "    if match:\n",
    "        try:\n",
    "            execute = json.loads(match.group(1))\n",
    "            if \"name\" in execute:\n",
    "                execute[\"function\"] = execute[\"name\"]\n",
    "            return execute\n",
    "        except:  # noqa: E722\n",
    "            pass\n",
    "        \n",
    "    return None\n",
    "\n",
    "func_execution = get_function_calling(raw_response)\n",
    "print(\"\\nWyodrębnione wywołanie funkcji:\")\n",
    "print(json.dumps(func_execution, indent=2, ensure_ascii=False) if func_execution else \"Nie znaleziono wywołania funkcji\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d9cb3f",
   "metadata": {},
   "source": [
    "## Function execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a392ed39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wykonuję funkcję: get_weather\n",
      "Z argumentami: {\"city\": \"Bydgoszcz\"}\n",
      "\n",
      "Wynik funkcji:\n",
      "{\n",
      "  \"temperatura\": \"23°C\",\n",
      "  \"opis\": \"słonecznie\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "if func_execution and \"function\" in func_execution and \"arguments\" in func_execution:\n",
    "    function_name = func_execution[\"function\"]\n",
    "    arguments = func_execution[\"arguments\"]\n",
    "\n",
    "    print(f\"\\nWykonuję funkcję: {function_name}\")\n",
    "    print(f\"Z argumentami: {json.dumps(arguments, ensure_ascii=False)}\")\n",
    "\n",
    "    try:\n",
    "        # Używamy eval() aby wywołać funkcję z argumentami\n",
    "        result = eval(f\"{function_name}(**{arguments})\")\n",
    "    except Exception as e:\n",
    "        result = {\"błąd\": str(e)}\n",
    "        print(f\"\\nBłąd: {str(e)}\")\n",
    "else:\n",
    "    result = None\n",
    "    print(\"\\nNie można wykonać funkcji - brak poprawnego wywołania.\")\n",
    "\n",
    "if result is not None:\n",
    "    print(\"\\nWynik funkcji:\")\n",
    "    print(json.dumps(result, indent=2, ensure_ascii=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755cbc3e",
   "metadata": {},
   "source": [
    "## Send funtion execution result to llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "68f3f9f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ostateczna odpowiedź modelu (z wynikiem funkcji):\n",
      "<s><s><|im_start|>   system\n",
      "Jesteś pomocnym asystentem AI. Gdy potrzebujesz dodatkowych informacji, możesz używać dostępnych funkcji.<|im_end|>  \n",
      "<|im_start|>   user\n",
      "Jaka jest pogoda w Bydgoszczy?\n",
      "\n",
      "Dostępne funkcje: [{\"name\": \"get_weather\", \"description\": \"Sprawdza aktualną pogodę w podanym polskim mieście\", \"parameters\": {\"type\": \"object\", \"properties\": {\"city\": \"string\", \"description\": \"Nazwa miasta w Polsce, np. Warszawa\"}}, \"required\": [\"city\"]}]<|im_end|>  \n",
      "<|im_start|>   assistant\n",
      "Aby uzyskać informacje o pogodzie w Bydgoszczy, użyję funkcji `get_weather` z miastem \"Bydgoszcz\".\n",
      "\n",
      "<tool_call>  {\"name\": \"get_weather\", \"arguments\": {\"city\": \"Bydgoszcz\"}}</tool_call><|im_end|> \n",
      "<|im_start|>  tool\n",
      "{\"temperatura\": \"23°C\", \"opis\": \"słonecznie\"}<|im_end|> \n",
      "<|im_start|>  assistant\n",
      "Aktualna pogoda w Bydgoszczy jest słoneczna z temperaturą 23°C.<|im_end|>\n",
      "\n",
      "\n",
      "-------------------------------\n",
      "\n",
      "\n",
      "\n",
      "Ostateczna odpowiedź (oczyszczona):\n",
      "Aktualna pogoda w Bydgoszczy jest słoneczna z temperaturą 23°C.\n"
     ]
    }
   ],
   "source": [
    "if result is not None:\n",
    "    # Tworzymy nowy prompt zawierający całą historię\n",
    "    final_prompt = (\n",
    "        f\"{raw_response}\\n\"\n",
    "        f\"<|im_start|> tool\\n\"\n",
    "        f\"{json.dumps(result, ensure_ascii=False)}<|im_end|>\\n\"\n",
    "        f\"<|im_start|> assistant\\n\"\n",
    "    )\n",
    "\n",
    "    final_inputs = tokenizer(final_prompt, return_tensors=\"pt\").to(device)\n",
    "\n",
    "    final_outputs = model.generate(final_inputs[\"input_ids\"], max_new_tokens=500)\n",
    "\n",
    "    final_response = tokenizer.decode(final_outputs[0], skip_special_tokens=False)\n",
    "    print(\"\\nOstateczna odpowiedź modelu (z wynikiem funkcji):\")\n",
    "    print(final_response)\n",
    "\n",
    "    print(\"\\n\\n-------------------------------\\n\\n\")\n",
    "    # Wyodrębniamy tylko ostatnią odpowiedź asystenta\n",
    "    import re\n",
    "    pattern = r'<\\|im_start\\|\\>\\s+assistant\\n(.*?)<\\|im_end\\|>'\n",
    "    matches = re.findall(pattern, final_response, re.DOTALL)\n",
    "\n",
    "    if matches:\n",
    "        last_assistant = matches[-1].strip()  # Bierzemy ostatnie dopasowanie\n",
    "        print(\"\\nOstateczna odpowiedź (oczyszczona):\")\n",
    "        print(last_assistant)\n",
    "    else:\n",
    "        print(\"\\nNie znaleziono odpowiedzi asystenta\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
